https://acg.cis.upenn.edu/milom/cis371-Spring08/lectures/05_fp.pdf
https://www.cs.umd.edu/~meesh/411/CA-online/chapter/floating-point-arithmetic-unit/index.html
https://eng.libretexts.org/Bookshelves/Computer_Science/Programming_Languages/x86-64_Assembly_Language_Programming_with_Ubuntu_(Jorgensen)/18%3A_Floating-Point_Instructions


Understanding floating point arithmetic and how it's carried out in IEEE 754 floating-point units (FPUs) is an in-depth topic, but here's a structured approach to help you understand each step of the process:
1. Learn the IEEE 754 Standard

The IEEE 754 standard defines the representation and operations for floating point numbers. Floating point numbers consist of three parts:

    Sign bit (1 bit): Determines if the number is positive or negative.
    Exponent (8 bits for single precision, 11 bits for double precision): Determines the scale of the number.
    Fraction (Mantissa, 23 bits for single precision, 52 bits for double precision): Holds the precision of the number.

For example, in single precision:

[ S | Exponent (8 bits) | Mantissa (23 bits) ]

    S = Sign bit (0 for positive, 1 for negative)
    Exponent = 8 bits
    Mantissa = 23 bits

The value is computed as:
(−1)sign×1.M×2(E−bias)
(−1)sign×1.M×2(E−bias)

Where:

    sign = sign bit (0 or 1)
    M = Mantissa (with an implicit leading 1 for normalized numbers)
    E = Exponent (with a bias depending on the precision: 127 for single precision, 1023 for double precision)
    bias = 127 (single) or 1023 (double)

2. Understand Floating Point Representation in Memory

    Learn how floating point numbers are stored in memory (as 32-bit or 64-bit binary values).
    Normalization: The mantissa is normalized (i.e., the first digit is always 1 unless it's zero), and the exponent is adjusted accordingly.
    Special cases: Zero, infinity, NaN (Not a Number), denormalized numbers, and subnormal numbers (very small numbers close to zero).

3. Steps in Floating Point Arithmetic

IEEE 754 floating-point operations, such as addition, subtraction, multiplication, and division, involve several steps. Here's a breakdown:
a. Addition/Subtraction

    Align the exponents: The exponents of the two numbers involved in the operation must be the same. This involves shifting the mantissa of the smaller number to match the larger number's exponent.
    Perform the operation: Once the exponents are aligned, the mantissas are added or subtracted (depending on the operation).
    Normalize the result: After the operation, the result might need to be normalized by adjusting the exponent and mantissa so that the leading bit is 1.
    Round the result: The result is rounded to the nearest representable value, following IEEE 754 rounding rules.

b. Multiplication

    Multiply the mantissas: The mantissas are multiplied.
    Add the exponents: The exponents are added (since multiplication of numbers means their exponents are added).
    Normalize the result: Normalize the resulting product, ensuring the leading bit of the mantissa is 1.
    Round the result: Round the result to the nearest representable value.

c. Division

    Divide the mantissas: The mantissas are divided.
    Subtract the exponents: The exponent of the divisor is subtracted from the exponent of the dividend.
    Normalize the result: Normalize the result so that the leading bit of the mantissa is 1.
    Round the result: Round the result according to IEEE 754 rules.

4. IEEE 754 Exceptions and Edge Cases

Understand the special cases defined by IEEE 754:

    Overflow: When the result is too large to represent.
    Underflow: When the result is too small to represent.
    NaN (Not a Number): Produced by undefined operations (e.g., 0/0, √-1).
    Infinity: Produced by operations like dividing by zero.
    Denormalized numbers: Numbers closer to zero than the smallest normalized number.

5. FPUs (Floating Point Units)

FPUs are hardware components that execute floating point arithmetic operations. The implementation of floating point operations in an FPU follows the IEEE 754 standard, and involves:

    Exponent alignment: Aligning the exponents of operands.
    Mantissa multiplication/division: Using hardware multipliers/dividers.
    Normalization: Hardware logic for shifting the mantissa and adjusting the exponent.
    Rounding: Using round mode logic to handle rounding errors.

6. Understand Rounding Modes

IEEE 754 defines four rounding modes:

    Round to nearest (default): Round to the nearest value, with ties rounded to the nearest even number.
    Round towards zero: Round towards zero (truncate).
    Round towards positive infinity: Round upwards.
    Round towards negative infinity: Round downwards.

7. Simulate Floating Point Arithmetic

    Software Simulation: To deeply understand floating point arithmetic, you can simulate it in software. Write a program that mimics the behavior of floating point operations according to the IEEE 754 rules. This includes handling special cases, rounding, and performing normalization.
    Study the FPU: If you have access to a hardware simulator or detailed architecture documentation, study how the FPU of a particular processor implements the IEEE 754 standard.

8. Practice with Examples

    Manual Calculation: Try converting decimal numbers to IEEE 754 representation, perform the operations (like addition, multiplication), and convert back to decimal. This will help you see how each step is performed.
    FP Operation Example: Let's say you want to add two numbers in IEEE 754 format:
        Convert the numbers to their IEEE 754 representation.
        Align the exponents.
        Add the mantissas.
        Normalize the result.
        Round the result.
        Convert the final result back to decimal.

9. Study FPUs in Specific Hardware Architectures

If you're interested in understanding FPUs in specific processors (like Intel, ARM, etc.), read the hardware manuals for those processors. Many of these processors have documentation detailing how floating-point operations are carried out, often including specifics about the FPU, rounding, and handling of exceptions.
10. Use Debuggers and Tools

Some debugging tools (e.g., GDB) and simulators allow you to step through floating point operations at the instruction level, showing how FPUs handle floating-point instructions. This will help you see how hardware performs these operations at a lower level.
Recommended Resources:

    IEEE 754 Standard (Official Documentation): To get an authoritative understanding of the standard itself.
    Computer Architecture Textbooks: Books like "Computer Organization and Design" by David Patterson and John Hennessy, which cover floating point units and IEEE 754.
    FPGA/ASIC Documentation: For hardware implementations of FPUs.
    Online Simulators: Tools like floating-point calculators and simulation environments can help you experiment with floating-point operations interactively.

By following these steps and practicing, you'll be able to understand the intricacies of floating-point arithmetic and how it's implemented in IEEE 754 FPUs.



///

A floating-point number in computer systems is used to represent real numbers in a way that can support a wide range of values. Floating-point numbers have several attributes that define their structure and precision. These attributes include:
1. Base (Radix)

    Definition: The base (or radix) of a floating-point number refers to the number system used for the representation. It is the foundational number system that governs how the number is expressed.
    Base for Floating-Point Numbers:
        In most modern computers, floating-point numbers are typically represented in binary (base 2). This means that numbers are stored as binary fractions.
        In some systems (though rare in practice), floating-point numbers might be represented in decimal (base 10), but binary is the standard in most systems, especially in IEEE 754 formats.

2. Precision

    Definition: Precision refers to the number of significant digits (bits or decimal places) that can be stored and represented accurately.
    Base Precision:
        In binary systems, precision is often measured in bits. For example:
            Single-precision floating-point numbers in IEEE 754 use 32 bits.
            Double-precision floating-point numbers in IEEE 754 use 64 bits.
    Decimal Precision: In decimal systems, precision would be measured in decimal places.

Attributes of Precision in Floating-Point Representation

    Significant Digits: This is the number of digits in the mantissa (also called the significand) that can be stored.
        For single precision (32-bit), the mantissa typically stores 23 bits of precision.
        For double precision (64-bit), the mantissa stores 52 bits of precision.
    The more bits used for the mantissa, the higher the precision of the floating-point number. However, there is always a limit to the number of bits that can be used, and this leads to rounding errors.

3. Exponent Range

    Definition: The exponent determines the scale of the floating-point number (how large or small the number can be).
    Range: Floating-point numbers in binary have a biased exponent, which can represent both very large and very small values. The range of exponents is determined by the number of bits allocated to the exponent field.
        For single precision (32 bits), the exponent is 8 bits long, and its range is from −126−126 to 127127 (after considering the bias).
        For double precision (64 bits), the exponent is 11 bits long, and its range is from −1022−1022 to 10231023.

4. Sign Bit

    Definition: The sign bit is a single bit used to indicate the sign (positive or negative) of the floating-point number.
    Single-Precision: The sign bit is 1 bit.
    Double-Precision: The sign bit is also 1 bit.

A 0 in the sign bit represents a positive number, while a 1 represents a negative number.
5. Normalization

    Definition: Normalization refers to the process of adjusting the representation of a floating-point number so that the leading digit (bit) of the mantissa is non-zero (in base-2 representation, it’s usually 1). This is done to maximize the precision of the number.
    Normalized Format:
        In binary, the leading bit of the mantissa is always 1 (but is not stored explicitly, it’s implicit). This allows the number to use the maximum possible precision.
        For example, the number 1.23×1031.23×103 would be represented in normalized form as 1.23×1031.23×103, and not as 0.123×1040.123×104.

6. Exponent Bias

    Definition: In the IEEE 754 standard, the exponent is stored in biased form to simplify the comparison of exponents. The bias ensures that both positive and negative exponents can be represented as unsigned integers.
    Formula for Bias:
    Bias=2(k−1)−1
    Bias=2(k−1)−1 where kk is the number of bits used for the exponent.
    Single-Precision: For an 8-bit exponent, the bias is 127127.
    Double-Precision: For an 11-bit exponent, the bias is 10231023.
    This bias allows the exponent field to handle both positive and negative exponents efficiently.

7. Denormals (Subnormals)

    Definition: Denormal numbers (also known as subnormal numbers) are numbers that are too small to be represented in the normalized form. They allow for a representation of very small numbers (close to zero), but at a cost of precision.
    When do they occur?: Denormals are used when the exponent is at its minimum value (after applying the bias) and the mantissa still contains non-zero values. Denormals allow a gradual underflow to zero.
    Impact on Precision: Denormals can be less precise because the exponent is no longer normalized, leading to fewer significant bits in the mantissa.

8. Machine Epsilon

    Definition: Machine epsilon is the smallest difference between two distinct floating-point numbers that can be represented in a given system. It is a measure of the precision of the floating-point representation.
    Formula:
    ϵ=2−(p−1)
    ϵ=2−(p−1) where pp is the number of bits in the mantissa (for example, p=24p=24 for single precision).
    Machine epsilon helps in understanding the accuracy of numerical calculations performed using floating-point numbers.

Summary of Floating-Point Number Attributes
Attribute	Explanation	Example (IEEE 754)
Base (Radix)	The number system used for the representation (binary, decimal).	Binary (base 2)
Precision	The number of significant digits (bits or decimal places) in the representation.	23 bits (single), 52 bits (double)
Exponent Range	The range of values that the exponent can represent (determined by bits).	Exponent range: −126−126 to 127127 (single)
Sign Bit	Indicates whether the number is positive or negative.	1 bit for sign
Normalization	Adjusts the mantissa to maximize precision, leading digit is non-zero.	Implicit leading 1 in mantissa
Exponent Bias	A bias is added to the exponent to simplify storage and comparison.	Bias: 127 (single), 1023 (double)
Denormal Numbers	Numbers smaller than the smallest normalized number, used for values close to zero.	Example: 2−1262−126 in single precision
Machine Epsilon	The smallest representable difference between two floating-point numbers.	For single precision: ϵ≈2−24ϵ≈2−24

In conclusion, floating-point numbers in computer systems have a complex structure that involves several attributes like base, precision, sign, exponent, and bias, all of which play an important role in ensuring that the numbers can represent a wide range of values with reasonable accuracy.


